{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1jtNXeC4cMMsc6D-Y851HQ6WpQRC9Hk65",
      "authorship_tag": "ABX9TyMyblEqE0acS1pH6XY5Tq2R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adithyanum/legal_document_simplifier/blob/main/legal_document_simplifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“˜ Legal Document Simplifier using BART and Streamlit\n"
      ],
      "metadata": {
        "id": "D3aVPMvPslZ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŽ¯ Objective\n",
        "\n",
        "The goal of this project is to create an interactive web app that allows users to upload legal documents in `.txt`, `.pdf`, or `.docx` format and receive simplified summaries of the text.\n",
        "\n",
        "This is especially useful for:\n",
        "- Law students\n",
        "- Researchers\n",
        "- Laypersons needing clarity on complex contracts or legal acts\n"
      ],
      "metadata": {
        "id": "UlpQTbyqsn_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ”§ Tools and Libraries\n",
        "\n",
        "- **Streamlit**: To build the interactive web UI\n",
        "- **Hugging Face Transformers**: For abstractive summarization using `facebook/bart-large-cnn`\n",
        "- **NLTK**: For sentence tokenization (chunking)\n",
        "- **PyMuPDF (`fitz`)**: For extracting text from PDFs\n",
        "- **python-docx**: For extracting text from DOCX files\n"
      ],
      "metadata": {
        "id": "FHVNvn7fsqs7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§  How It Works\n",
        "\n",
        "1. User uploads a legal document (`.txt`, `.pdf`, or `.docx`)\n",
        "2. The document text is extracted based on file type\n",
        "3. If the text is short (less than 700 words), it's summarized directly\n",
        "4. If long, the document is split into chunks using `nltk.sent_tokenize()`\n",
        "5. Each chunk is summarized using BART (`facebook/bart-large-cnn`)\n",
        "6. All summaries are combined and shown to the user\n",
        "7. User can download the final summary as a `.txt` file\n"
      ],
      "metadata": {
        "id": "XkmFnSmvswYP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§ª Features\n",
        "\n",
        "- ðŸ“‚ Accepts `.txt`, `.pdf`, and `.docx` legal documents\n",
        "- âœ‚ï¸ Automatically chunks long documents\n",
        "- ðŸ§  Summarizes using Hugging Face BART model\n",
        "- ðŸŽ› Users can choose between \"Concise\" or \"Detailed\" summary styles\n",
        "- ðŸ“¥ Final summary is downloadable"
      ],
      "metadata": {
        "id": "R2yKpZaCsxrp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfRRr4S1v9XW"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import docx\n",
        "import fitz\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ¤– Model: facebook/bart-large-cnn\n",
        "\n",
        "- **Type**: Abstractive Summarizer\n",
        "- **Base**: BART (Bidirectional and Auto-Regressive Transformer)\n",
        "- **Fine-tuned on**: CNN/DailyMail news articles\n",
        "- **Strength**: Good at condensing long passages and preserving meaning\n"
      ],
      "metadata": {
        "id": "5pKrDi1UtB6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline('summarization', model ='facebook/bart-large-cnn')"
      ],
      "metadata": {
        "id": "HVRyY_tPwek2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMPQgG-Z1HGs",
        "outputId": "6a92e355-3e0d-4e9d-8be2-7cb995065836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ› ï¸ Sentence Chunking (Why?)\n",
        "\n",
        "Transformers like BART have a max token limit (typically 1024 tokens). To handle long legal documents, we:\n",
        "- Split the text into sentences using `nltk.sent_tokenize()`\n",
        "- Combine sentences into chunks of ~500â€“800 words\n",
        "- Summarize each chunk separately and merge results\n"
      ],
      "metadata": {
        "id": "WwKPhQNYs-Kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, max_words=800) :\n",
        "\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "\n",
        "    for sentence in sentences :\n",
        "        word_count = len(sentence.split())\n",
        "\n",
        "        if current_length + word_count > max_words :\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = [sentence]\n",
        "            current_length = word_count\n",
        "        else :\n",
        "            current_chunk.append(sentence)\n",
        "            current_length += word_count\n",
        "\n",
        "    if current_chunk :\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "wfJyGxIR8WBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_chunks(chunks) :\n",
        "    summaries = []\n",
        "\n",
        "    for chunk in chunks :\n",
        "        summary = summarizer(chunk, max_length = max_len, min_length = min_len, do_sample = False)[0]['summary_text']\n",
        "        summaries.append(summary)\n",
        "\n",
        "    return summaries"
      ],
      "metadata": {
        "id": "IIRL8fSeBfLh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st"
      ],
      "metadata": {
        "id": "0n0kK0Ayw4F5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st.title('ðŸ“„ Legal Document Simplifier')\n",
        "st.write('Upload a Legal Document to get a simplified summary.')"
      ],
      "metadata": {
        "id": "MIA_Z4Iv0Dhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded_file = st.file_uploader(\"Upload a Legal Document ðŸ“‚\", type = ['txt','pdf','docx'])"
      ],
      "metadata": {
        "id": "aZcSx54I0Pog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_file):\n",
        "    text = \"\"\n",
        "    with fitz.open(stream=pdf_file.read(), filetype=\"pdf\") as doc:\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "    return text"
      ],
      "metadata": {
        "id": "w7EIquVG0he3"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_docx(docx_file):\n",
        "    text = \"\"\n",
        "    document = docx.Document(docx_file)\n",
        "    for paragraph in document.paragraphs:\n",
        "        text += paragraph.text + \"\\n\"\n",
        "    return text"
      ],
      "metadata": {
        "id": "BdI_zbMW1HHk"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if uploaded_file is not None :\n",
        "    file_extension = os.path.splitext(uploaded_file.name)[1].lower()\n",
        "\n",
        "    if file_extension == '.txt':\n",
        "        raw_text = uploaded_file.read().decode(\"utf-8\")\n",
        "    elif file_extension == '.pdf':\n",
        "        raw_text = extract_text_from_pdf(uploaded_file)\n",
        "    elif file_extension == '.docx':\n",
        "        raw_text = extract_text_from_docx(uploaded_file)\n",
        "    else:\n",
        "        st.error(\"Unsupported file type.\")\n",
        "        raw_text = None\n",
        "\n",
        "    if raw_text:\n",
        "        st.subheader(\"ðŸ“„ Original Legal Document:\")\n",
        "        st.text_area(\"Full Text\", raw_text, height=300)\n",
        "\n",
        "        summary_style = st.selectbox(\"Select Summary Style\", [\"Concise\", \"Detailed\"])\n",
        "        if summary_style == \"Concise\":\n",
        "            max_len, min_len = 150, 30\n",
        "        else:\n",
        "            max_len, min_len = 300, 50\n",
        "\n",
        "\n",
        "        if st.button(\"Simplify Document\"):\n",
        "            with st.spinner(\"Summarizing... Please wait...\"):\n",
        "                if len(raw_text.split()) < 700:\n",
        "                    summary = summarizer(raw_text, max_length=250, min_length=50, do_sample=False)[0]['summary_text']\n",
        "                    final_summary = summary\n",
        "                    st.subheader(\"âœ‚ï¸ Simplified Summary:\")\n",
        "                    st.text_area(\"Summary\", summary, height=300)\n",
        "                else:\n",
        "                    chunks = chunk_text(raw_text)\n",
        "                    summaries = summarize_chunks(chunks)\n",
        "                    final_summary = \"\\n\".join(summaries)\n",
        "                    st.subheader(\"âœ‚ï¸ Simplified Summary:\")\n",
        "                    st.text_area(\"Summary\", final_summary, height=300)\n",
        "\n",
        "                st.download_button(\"ðŸ“¥ Download Summary\", final_summary, file_name=\"summary.txt\")"
      ],
      "metadata": {
        "id": "04EGU7rB1OiV"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“Š Results\n",
        "\n",
        "The Legal Document Simplifier successfully summarized long and complex legal documents â€” including the **Patents Act, 1970 (India)** â€” into concise, readable summaries using BART-based abstractive summarization.\n",
        "\n",
        "### ðŸ§ª Example:\n",
        "\n",
        "**Original Document**: ~14,000 words  \n",
        "**Simplified Summary**: ~300â€“500 words\n",
        "\n",
        "### ðŸ“ Observations:\n",
        "\n",
        "- âœ… Retains essential legal terms (e.g., \"patent\", \"specification\", \"controller\")\n",
        "- âœ… Condenses redundant clauses and long procedural paragraphs\n",
        "- âœ… Performs best on acts, contracts, and legal notices with formal structure\n",
        "- âš ï¸ May retain original phrasing for legally sensitive definitions (intentional behavior of BART)\n",
        "\n",
        "This tool significantly reduces the time and effort needed to interpret legal documents while preserving critical information.\n"
      ],
      "metadata": {
        "id": "CAvAAjVw0VAq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“Ž Sample Output\n",
        "\n",
        "[ðŸ“„ View Sample Summary PDF](https://github.com/adithyanum/legal_document_simplifier/blob/977c317e1ec518e2081a9ebb225f30b3c1931995/Legal_Document_Simplifier.pdf)\n"
      ],
      "metadata": {
        "id": "-Qk_Pyikur-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“Œ Conclusion\n",
        "\n",
        "This app simplifies complex legal documents using cutting-edge NLP. It is a helpful tool for anyone who needs to understand legal content more easily and quickly."
      ],
      "metadata": {
        "id": "4iSnA-natXXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ™ Acknowledgments\n",
        "\n",
        "I would like to acknowledge the following tools, organizations, and libraries for making this project possible:\n",
        "\n",
        "- ðŸ¤— **Hugging Face Transformers** â€” for providing the `facebook/bart-large-cnn` pretrained summarization model\n",
        "- ðŸ“š **NLTK (Natural Language Toolkit)** â€” for sentence-level tokenization\n",
        "- ðŸ“„ **PyMuPDF** and **python-docx** â€” for extracting text from PDFs and DOCX files\n",
        "- ðŸš€ **Streamlit** â€” for enabling rapid development of user-friendly NLP apps\n",
        "- ðŸ‘¨â€âš–ï¸ Indian Government Publications â€” for access to the **Patents Act, 1970** as a test document\n",
        "\n",
        "Special thanks to the open-source community for building tools that make applied NLP accessible to everyone.\n"
      ],
      "metadata": {
        "id": "aEK-KvIB0iYf"
      }
    }
  ]
}